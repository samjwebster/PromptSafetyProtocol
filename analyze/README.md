# `/analyze`

This folder contains scripts used to analyze the responses generated by baseline LLM configuration and all tested report modes and rejection mechanisms.

`get_subset.py`: Gets a subset of 100 random prompt responses to be hand analyzed for the baseline config as well as the concise and verbose report modes. These selected subsets are class balanced: 25 jailbreaking, 25 prompt extraction, 25 prompt injection, and 25 safe prompts. They are compiled with the prompt, the prompt's id and label, the LLM response, and two fields to be manually filled out: whether the LLM accepted or rejected that prompt in its response, and if the safety report was leaked. 

`subsets/`: Contains the subsets created by the script above, with manual annotations of how the LLM performed in the named configurations.

`analyze_subsets.py`: Looks at the fully labeled subsets to analyze accuracy and leakage more closely, such as by prompt class.

`get_rejection_and_aucs.py`: Computes the AUC (and conf. matrix values), accuracy, and leakage scores for the tested rejection mechanisms. It also computes the AUC metrics for the subsets described above.